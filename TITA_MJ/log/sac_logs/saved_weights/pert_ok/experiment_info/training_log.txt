Initial test step: test_reward: 4.705614 ± 0.000000, best_reward: 4.705614 ± 0.000000 in #0
Epoch #1: test_reward: 2.101265 ± 0.000000, best_reward: 4.705614 ± 0.000000 in #0
Epoch #2: test_reward: 3.102189 ± 0.000000, best_reward: 4.705614 ± 0.000000 in #0
Epoch #3: test_reward: 4.281650 ± 0.000000, best_reward: 4.705614 ± 0.000000 in #0
Epoch #4: test_reward: 3.956601 ± 0.000000, best_reward: 4.705614 ± 0.000000 in #0
Epoch #5: test_reward: 4.415977 ± 0.000000, best_reward: 4.705614 ± 0.000000 in #0
Epoch #6: test_reward: 1.131967 ± 0.000000, best_reward: 4.705614 ± 0.000000 in #0
Epoch #7: test_reward: 2.542953 ± 0.000000, best_reward: 4.705614 ± 0.000000 in #0
Epoch #8: test_reward: 4.688639 ± 0.000000, best_reward: 4.705614 ± 0.000000 in #0
Epoch #9: test_reward: 3.388986 ± 0.000000, best_reward: 4.705614 ± 0.000000 in #0
Epoch #10: test_reward: 4.569822 ± 0.000000, best_reward: 4.705614 ± 0.000000 in #0
Epoch #11: test_reward: 4.527569 ± 0.000000, best_reward: 4.705614 ± 0.000000 in #0
Epoch #12: test_reward: 4.615425 ± 0.000000, best_reward: 4.705614 ± 0.000000 in #0
Epoch #13: test_reward: 4.699284 ± 0.000000, best_reward: 4.705614 ± 0.000000 in #0
Epoch #14: test_reward: 4.460773 ± 0.000000, best_reward: 4.705614 ± 0.000000 in #0
Epoch #15: test_reward: 1.851468 ± 0.000000, best_reward: 4.705614 ± 0.000000 in #0
Epoch #16: test_reward: 3.530012 ± 0.000000, best_reward: 4.705614 ± 0.000000 in #0
Epoch #17: test_reward: 4.404508 ± 0.000000, best_reward: 4.705614 ± 0.000000 in #0
Epoch #18: test_reward: 4.490687 ± 0.000000, best_reward: 4.705614 ± 0.000000 in #0
Epoch #19: test_reward: 4.453208 ± 0.000000, best_reward: 4.705614 ± 0.000000 in #0
Epoch #20: test_reward: 4.623324 ± 0.000000, best_reward: 4.705614 ± 0.000000 in #0
Epoch #21: test_reward: 4.481493 ± 0.000000, best_reward: 4.705614 ± 0.000000 in #0
Epoch #22: test_reward: 4.429859 ± 0.000000, best_reward: 4.705614 ± 0.000000 in #0
Epoch #23: test_reward: 4.713462 ± 0.000000, best_reward: 4.713462 ± 0.000000 in #23
Epoch #24: test_reward: 4.748353 ± 0.000000, best_reward: 4.748353 ± 0.000000 in #24
Epoch #25: test_reward: 4.438739 ± 0.000000, best_reward: 4.748353 ± 0.000000 in #24
Epoch #26: test_reward: 4.654465 ± 0.000000, best_reward: 4.748353 ± 0.000000 in #24
Epoch #27: test_reward: 4.849120 ± 0.000000, best_reward: 4.849120 ± 0.000000 in #27
Epoch #28: test_reward: 4.830387 ± 0.000000, best_reward: 4.849120 ± 0.000000 in #27
Epoch #29: test_reward: 4.433097 ± 0.000000, best_reward: 4.849120 ± 0.000000 in #27
Epoch #30: test_reward: 4.502308 ± 0.000000, best_reward: 4.849120 ± 0.000000 in #27
Epoch #31: test_reward: 4.276434 ± 0.000000, best_reward: 4.849120 ± 0.000000 in #27
Epoch #32: test_reward: 4.334925 ± 0.000000, best_reward: 4.849120 ± 0.000000 in #27
Epoch #33: test_reward: 4.509042 ± 0.000000, best_reward: 4.849120 ± 0.000000 in #27
Epoch #34: test_reward: 4.567849 ± 0.000000, best_reward: 4.849120 ± 0.000000 in #27
Epoch #35: test_reward: 4.507569 ± 0.000000, best_reward: 4.849120 ± 0.000000 in #27
Epoch #36: test_reward: 4.792508 ± 0.000000, best_reward: 4.849120 ± 0.000000 in #27
Epoch #37: test_reward: 4.852770 ± 0.000000, best_reward: 4.852770 ± 0.000000 in #37
Saved /home/ubuntu/Desktop/repo_rl/TITA-dynamic-obstacle-avoidance/TITA_MJ/log/sac_logs/weights/sac_day_2026_01_30_time_11_43_47/experiment_info/plots/plots_test/render_test_reward_info.png
