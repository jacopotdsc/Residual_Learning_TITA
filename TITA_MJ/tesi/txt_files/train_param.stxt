conda deactivate && cd /home/ubuntu/Desktop/repo_rl/TITA-dynamic-obstacle-avoidance/TITA_MJ/ && conda activate tianshou_gpu && export LD_PRELOAD=$CONDA_PREFIX/lib/libstdc++.so.6 && export OMP_NUM_THREADS=1
export LD_PRELOAD=$CONDA_PREFIX/lib/libstdc++.so.6 && export OMP_NUM_THREADS=1


Neural Network
    Actor = Critcc
        output: (mu, sigma) 
                mu = tanh(mu) * max_action, 
                sigma = param
        act_fn = tanh
        weight_init = check fn

PPO:
    learning_rate = 0.00001
    eps_clip = 0.1, ratio of change between new and old policy
    vf_coef = 0.5, weight the value loss relative to actor loss in the overall of loss fn
    ent_coef = 0.0, exploration
    gae_lambe = 0.90, acting as a weighting factor for combining different n-step advantage estimators
    max_grad_norm = 0.5, maximum L2 norm threshold for gradient clipping
    gamma = 0.995, discount factor for future reward
    
Buffer: It is used for storing transition from different environments yet keeping the order of time
    tota_size = 1000*num_train_env, total size of Buffer
    buffer_num = num_train_env, number of ReplayBuffer it uses,

Training:
    train_env = 16
    test_env = 4
    normalize_observation = true, check 
    
    max_epoch = 100
    batch_size = 512
    epoch_num_step = 20_000, total steps per epoch_num_step
    collection_step_num_env_step = 256*train_env, transition to collect before update
    update_step_num_repetion = 10, updates after collection with same data
    test_step_num = test_env

-------------

offline_trainer = OffPolicyTrainerParams(
            training_collector=train_collector, 
            test_collector=test_collector,  
            logger=logger,
            save_best_fn=partial(save_best, alg_type=alg_type, actor_path=actor_path, critic_path=critic_path),
            
            test_in_training=False,

            # Know parameters 
            max_epochs=15,   
            batch_size=1024,

            # Total number of training steps to take per epoch
            epoch_num_steps=10*num_training_envs, 

            # the number of environment steps/transitions to collect in each collection step before the
            # network update within each training step.
            collection_step_num_env_steps=10*num_training_envs,
            #collection_step_num_episodes=1*num_training_envs 
            
            # The number of times data 
            update_step_num_gradient_steps_per_sample=10,

            # Number of episodes to colleact in each test step
            # i.e. number of run for evaluation
            test


Critic module device: cuda:0
Starting training enviroment Tita-v0
Initial test step: test_reward: 2035.364589 ± 0.000000, best_reward: 2035.364589 ± 0.000000 in #0
Epoch #1: 100%|####################################################################################################| 160/160 [00:46<00:00,  3.42it/s, env_episode=0, env_step=160, n_ep=0, n_st=160, update_step=1]
Epoch #1: test_reward: 1863.962029 ± 0.000000, best_reward: 2035.364589 ± 0.000000 in #0###########################| 160/160 [00:46<00:00,  3.42it/s, env_episode=0, env_step=160, n_ep=0, n_st=160, update_step=1]
Epoch #2: 100%|####################################################################################################| 160/160 [00:46<00:00,  3.42it/s, env_episode=0, env_step=320, n_ep=0, n_st=160, update_step=2]
Epoch #2: test_reward: 2913.722595 ± 0.000000, best_reward: 2913.722595 ± 0.000000 in #2###########################| 160/160 [00:46<00:00,  3.42it/s, env_episode=0, env_step=320, n_ep=0, n_st=160, update_step=2]
Epoch #3: 100%|####################################################################################################| 160/160 [00:47<00:00,  3.35it/s, env_episode=0, env_step=480, n_ep=0, n_st=160, update_step=3]
Epoch #3: test_reward: 2930.131228 ± 0.000000, best_reward: 2930.131228 ± 0.000000 in #3###########################| 160/160 [00:47<00:00,  3.35it/s, env_episode=0, env_step=480, n_ep=0, n_st=160, update_step=3]
Epoch #4: 100%|####################################################################################################| 160/160 [00:48<00:00,  3.32it/s, env_episode=0, env_step=640, n_ep=0, n_st=160, update_step=4]
Epoch #4: test_reward: 2975.945340 ± 0.000000, best_reward: 2975.945340 ± 0.000000 in #4###########################| 160/160 [00:48<00:00,  3.32it/s, env_episode=0, env_step=640, n_ep=0, n_st=160, update_step=4]


---------------------------